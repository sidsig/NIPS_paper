\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{A Tractable Model for Generating High-Dimensional Sequences}


\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
We propose a tractable probabilistic model for generating complex high dimensional sequences. This model combines a powerful distribution estimator, the Real-Valued Neural Autoregressive Distribution Estimator (RNADE) with a Recurrent Neural Network (RNN) for capturing temporal dependencies in high-dimensional data. Maximum likelihood learning can be applied to efficiently train the model using a gradient based optimiser. Unlike other models in this family, log-likelihoods for sequences can be computed exactly and efficiently. We evaluate the model's performance on two standard datasets and compare its performance to other models. 
\end{abstract}

\section{Introduction}

Modeling sequences is a fundamental problem in machine learning. Many popular models like Hidden Markov Models (HMMs) and Linear Dynamical Systems use latent variables to capture temporal properties of sequential model. However HMMs employ a 1-of-K state representation for the hidden state and trying to model larger histories can lead to exponentially large hidden states. RNNs are an interesting alternative for modeling sequential data and have recently been employed in speech recognition, MIR and various other tasks. RNNs employ a deterministic distributed hidden representation that can in theory capture temporal properties over very long time scales. However RNNs are limited by the effectiveness of gradient based optimisers and the size of the output space that they can model. To rectify this issue, a family of models that condition complex distribution estimators on the hidden state of an RNN have been proposed. The first such model was the RTRBM and it was later extended by the RNN-RBM and the RNN-NADE \cite{Boulanger-Lewandowski2012}. 

The main idea behind this family of models is to use complex distribution estimators like the RBM and the NADE and incorporate a temporal element by conditioning the parameters of the model at each time step on previously observed parts of the sequence. As mentioned before an RNN can in theory learn dependencies over large time-scales within the data and is used in these models for conditioning the distributions at each step. RNN based models are also attractive because the recurrent parts of the model can be easily trained by using BPTT or Hessian Free (HF) optimisation. The RTRBM and the RNN-NADE models cannot be trained exactly by maximum likelihood learning because the gradients of the RBMs cannot be computed exactly. This problem was rectified by using a NADE instead of an RBM. However, this family of temporal models is not very well suited to modeling real-valued data. Both the RBM and the NADE estimate distributions over binary spaces. Although the RBM can be used on real data by either scaling the data between $[0,1]$ or by using the Gaussian-Bernouilli RBM (gRBM), we feel that using an estimator meant for real data could provide further gains. 

In this paper, we propose a model called the RNN-RNADE that combines the temporal modeling properties of the RNN with the recently proposed RNADE model. The RNADE is a generalisation of the NADE to real-valued data. It has been shown to outperform gaussian mixture models (GMMS) and many other distribution estimators on a variety of datasets of varying dimensionality and complexity. In this paper we explore how the RNADE can be combined with the RNN to yield a temporal generative model. We show how the model can be trained efficient using maximum likelihood learning using gradient based optimisers. Finally we evaluate the performance  

%Recently various neural based models have been proposed for learning modelling high dimensional sequential data. These models have been shown to work much better than standard models such as HMMs and Linear Dynamical Systems. These models include the TRBM, the RTRBM and the RNN-RBM. Although these models work very well for complex high dimensional data, they have several limitations. They cannot be trained exactly because their gradients can't be computed. Also obtaining probabilities from the model is not tractable and often very expensive. These models also suffer from the fact, that the distribution estimators they use are meant for binary data. Even though these models can be extended to model real-value with some approximations, these models are known to have several short-comings. The RNN-NADE is a tractable alternative, however it is unsatisfactory, because the NADE can only model distributions over high-dimensional binary vectors. In this paper we propose a tractable distribution estimator for high-dimensional real-valued sequences by making use of the RNADE which was proposed recently. The proposed model can be trained and sampled from efficiently by back-propagating error gradients through the computational graph. A key advantage of this model over other neural-based sequence models is that getting the likelihoods of the occurrence of sequences is tractable and fast. Because of this, the model can be easily combined with other sequence models and is applicable to various tasks like sequence completion, sequence denoising and classification of complex real-valued sequences. We compare our model's performance to the earlier models and demonstrate its many benefits. 
 


\section{The RNADE}
\label{gen_inst}

The RNADE is a generalisation of the NADE to real-valued data. Like the NADE, the RNADE expresses the joint probability of the data, as a product of one-dimensional conditional distributions as follows:
$$ p(x) = \prod_{d=1}^{D} p(x_d|\mathbf{x_{<d}}) \: \text{with} \: p(x_d|\mathbf{x_{<d}}) = p_{\mathcal{M}(x_d|\theta_d)} $$ where $p_{\mathcal{M}}$ is a mixture of Gaussians and $\boldsymbol{x}_{<d}$ is a vector of all the dimensions of the data point $<d$. The RNADE models each of the conditional distributions by means of a feed-forward neural network with tied weights, with one neural network for each dimension. The activations of the hidden units are calculated as follows:

$$ \mathbf{a}_d = \boldsymbol{W}_{.,<d}\boldsymbol{x}_d + \mathbf{c}$$
$$ \boldsymbol{h}_d = \sigma (\rho_d \mathbf{a}_d)$$


where $\mathbf{c} \in \mathbb{R}^{H}$ and $\boldsymbol{W} \in \mathbb{R}^{D \times (H-1)}$ are neural network parameters that are shared across all the neural networks and $\sigma(x) = 1/(1+e^{-x})$ is the sigmoid function. $\boldsymbol{W}_{.,<d}$ represents the first $D-1$ columns of the shared weight matrix. The term $\rho_d$ is a scaling factor which is also learnt from the data. The scaling factor was introduced in \cite{AISTATS2011_Bengio11} in order to prevent the sigmoid hidden units from saturating. The computation of the activations of the hidden units can be made more efficient by performing the computation as:
$$ \mathbf{a}_1 = \mathbf{c}, \: \: \; \mathbf{a}_{d+1} = \mathbf{a}_{d} + x_d \mathbf{W}_{.,d}$$

 Unlike the NADE which models each output as a bernouilli distribution, the outputs of each of the feed forward neural networks of the RNADE are mixtures of Gaussians. Therefore the RNADE comprises of $D$ mixture density networks with tied input-to-hidden weights. Once the hidden units of the RNADE have been computed, they are used to compute the parameters of the GMMs $\boldsymbol{\theta}_d  = \left\{ \boldsymbol{\alpha}_d, \boldsymbol{\mu}_d, \boldsymbol{\sigma}_d \right\}$ at each output, where $\boldsymbol{\alpha}_d$ are the mixing coefficients, $\boldsymbol{\mu}_d$ are the means and $\boldsymbol{\sigma}_d$ are the variances. These parameters are computed as follows:
$$ \boldsymbol{\alpha}_d = \text{softmax} ({\mathbf{V}_{d}^{\alpha}}^T \mathbf{h}_d + \mathbf{b}^{\alpha}_{d})$$
$$ \boldsymbol{\mu}_d = {\mathbf{V}_{d}^{\mu}}^T \mathbf{h}_d + \mathbf{b}^{\mu}_{d}$$
$$ \boldsymbol{\sigma}_d = \exp ({\mathbf{V}_{d}^{\sigma}}^T \mathbf{h}_d + \mathbf{b}^{\sigma}_{d})$$

where $ \mathbf{V}_{d}^{\alpha},\mathbf{V}_{d}^{\mu},\mathbf{V}_{d}^{\sigma}$ are $H \times K$ matrices , $\mathbf{b}^{\alpha}_{d},\mathbf{b}^{\mu}_{d},\mathbf{b}^{\sigma}_{d}$ are vectors of size $K$ and $K$ is the number of components in the GMM. The parameters of the RNADE can be learnt by performing gradient ascent on the log-likelihood of the training set. 

\section{RNN-based Models for Sequences}

An RNN can define a distribution over sequences $\mathbf{x}_1^{T}$ if the loss function is of the form $ L = \sum_{t} -\log p(\mathbf{x}^{t+1};\boldsymbol{\theta}^{t})$ where $p(.;\boldsymbol{\theta})$ is a distribution with parameters $\boldsymbol{\theta}$, and $\boldsymbol{\theta}^t \equiv f(\mathbf{x}_1^t)$. In more detail, according to figure 1, let the hidden state of the RNN at time $t$ be give by:
$$ \mathbf{h}_{RNN}^t = \sigma(\mathbf{W}_{in}\mathbf{x}^t + \mathbf{W}_{rec}\mathbf{h}_{RNN}^{t-1} + \mathbf{b}_{RNN})$$
Let the hidden state of the RNN at time $t-1$ be used to output $\mathbf{z}^{t} = \sigma(\mathbf{W}_{out}\mathbf{h}_{RNN}^{t-1} + \mathbf{b}_{out})$ and let the parameters of the distribution at time $t$, $\boldsymbol{\theta}^t = f(\mathbf{z}^t,\boldsymbol{\theta}_{model})$ where $\boldsymbol{\theta}_{model}$ are the independent parameters of the distribution estimator and $f$ is some differentiable function with respect to the model parameters. If we can obtain the derivates of the cost function with respect to the parameters of the distribution $p(;|\boldsymbol{\theta})$, then it can be shown that the parameters of the RNN can be trained by using the chain rule and Back Propagation Through Time (BPTT). Therefore:

$$ \frac {\partial L}{\partial{\theta^t}} = -\frac {\partial p(\mathbf{x}^t)}{\partial \theta^t}$$
$$ \frac {\partial L}{\partial{\theta_{model}}} = \sum_{t} -\frac {\partial p(\mathbf{x}^t)}{\partial{\theta_{model}}}   $$

$$ \frac {\partial L}{\partial{\mathbf{z}^t}} = \sum_{T} \sum_{i} -\frac {\partial p(\mathbf{x}^t)}{\partial{\theta^i}} \frac {\partial f(\mathbf{z}^t,\boldsymbol{\theta}_{model})^i}{\partial{\mathbf{z}^t}}  $$

Once we obtain all the errors with respect to the outputs of the RNN, the gradients with respect to the RNN parameters can be easily found by applying the chain rule further and backpropagating the gradients back in time. We show the calculations for the proposed model in section 3. Further derivations of the results can be found in the supplementary material. 

The RTRBM \cite{Sutskever2008} was the first model that uses the above idea to condition RBMs on the outputs of an RNN. However, the model is constrained by the fact that the outputs of RNN are responsible for both the density estimation and for propagating temporal information to the following time step. The RNN-RBM \cite{Boulanger-Lewandowski2012} generalises the RTRBM model by introducing a separate hidden layer for the RNN. Decoupling the states of the RNN and conditional RBMs led to an improvement in performance. 

The RNN-RBM model cannot be trained exactly because the error derivatives with respect to the parameters of the conditional RBMs are approximated using Contrastive Divergence (CD). Another drawback of the RNN-RBM model is that obtaining the log-likelihood of the sequence is intractable. This is because the RBM defines a joint distribution over the observed and hidden variables. Obtaining probabilities over the observed variables implies a summation over an exponential number of hidden states. These shortcomings can be addressed by replacing the RBM with the NADE, which is a tractable distribution estimator with performance comparable to RBMs. The log-likelihood can be easily calculated and more powerful gradient based optimisers like HF \cite{Martens2011} can be applied because the gradients are exactly calculable. 



\bibliography{bibliography}
\bibliographystyle{plain}
\end{document}
