


\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
%\usepackage{algpseudocode}
%\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Supplementary material for RNN-RNADE: A Tractable Model for High-Dimensional Real Valued Sequences}

\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

% \begin{abstract}
% This document contains some of the mathematical derivations for the RNN-RNADE model. 
% \end{abstract}

\section{Introduction}

This document contains supplementary material for the paper ``RNN-RNADE: A Tractable Model for High-Dimensional Real Valued Sequences''. Section 2 describes an algorithm for calculating the gradients of the joint RNN-RNADE model and updating the parameters of the model based on these gradients. The last 2 sections describe videos that are included with this supplementary material, which illustrate the model's performance. 

\section{Training Algorithm}
% The RNADE fprop:
% $$ p(x) = \prod_{d=1}^{D} p(x_d|\mathbf{x_{<d}}) \: \text{with} \: p(x_d|\mathbf{x_{<d}}) = p_{\mathcal{M}(x_d|\theta_d)} $$
% $$ \mathbf{a}_{d+1} = \mathbf{a}_{d} + x_d\mathbf{W.,}_{d}$$
% $$ \mathbf{h_d} = \text{sigm}(\rho_d \mathbf{a_d})$$
% $$ \boldmath{\alpha}_d = \text{softmax}(\mathbf{V_{d}^{\alpha}h_d} + \mathbf{b_{d}^{\alpha}})$$
% $$ \boldmath{\mu}_d = \mathbf{V}_{d}^{\mu}\mathbf{h}_d + \mathbf{b}_{d}^{\mu}$$
% $$\boldmath{\sigma}_d = \text{exp}(\mathbf{V}_{d}^{\sigma}\mathbf{h}_d + \mathbf{b}_{d}^{\sigma})$$


% \begin{algorithm}
% \caption{RNADE fprop}\label{euclid}
% \begin{algorithmic}%[1]
% \Procedure{fprop}{}
% \State $a \gets c$
% \State $p \gets 1$
% \For{$d$ from $1$ to $D$}
% \State $\mathbf{\psi_d} = \mathbf{\rho_d} \mathbf a$
% \State $\mathbf{h_d} = \text{sigm}(\mathbf{\psi_d})$
% \State $\mathbf{z}_{d}^{\alpha} = \mathbf{{V_{d}^{\alpha}}^{T}h_d} + \mathbf{b_{d}^{\alpha}} $
% \State $\mathbf{z}_{d}^{\mu} = \mathbf{{V_{d}^{\mu}}^{T}h_d} + \mathbf{b_{d}^{\mu}}$
% \State $\mathbf{z}_{d}^{\sigma} = \mathbf{{V_{d}^{\sigma}}^{T}h_d} + \mathbf{b_{d}^{\sigma}}$
% \State $\boldmath{\alpha_d} = \text{softmax}(\mathbf{z}_{d}^{\alpha})$
% \State $\boldmath{\mu_d} = \mathbf{z}_{d}^{\mu}$
% \State $\boldmath{\sigma_d} = \text{exp}(\mathbf{z}_{d}^{\sigma})$
% \State $p(\mathbf{x}) = p(\mathbf{x})p_{\mathcal{M}} (x_d;\boldmath{\alpha_d};\boldmath{\mu_d};\boldmath{\sigma_d})$
% \State $ \mathbf{a} = \mathbf{a} + x_d\mathbf{W.,}_{d}$
% \EndFor
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}
This section describes the algorithm for training the RNN-RNADE using back-propagation. Further details of the derivation of the gradients for the RNADE can be found in the supplementary material for the original RNADE paper \cite{Uria2013}. We first present the algorithm for the calculation of the gradients for the RNADE. Our gradients differ slightly from the gradients in \cite{Uria2013}, because we use sigmoid activation functions for the RNADE instead of rectified linear activations \cite{Uria2013}. In our initial experiments with \textit{mocap} dataset, we found that the training algorithm for the joint model was more stable and less prone to exploding gradients, when sigmoid activations were used for the RNADE. 

Algorithm 1 shows how the gradients for the RNADE are computed. Algorithm 2 then shows how the gradients from the RNADE can be used to train the joint model. 

\begin{algorithm}
\caption{RNADE gradients}
\begin{algorithmic}%[1]
\State $\mathbf{a} \gets \mathbf{c}$
\For{$d$ from $1$ to $D$}
	\State $\mathbf{a} \gets \mathbf{a} + x_{d}\mathbf{W}_{.,d} $
\EndFor
\For{$d$ from $D$ to $1$}
\State $\boldsymbol{\psi} \gets \rho_{d} \mathbf a$
\State $\mathbf{h} \gets \sigma(\boldsymbol{\psi})$
\State $\mathbf{z}^{\alpha} \gets {\mathbf{V}^{\alpha}_{d}}^{T}\mathbf{h} + \mathbf{b}^{\alpha}_{d} $
\State $\mathbf{z}^{\mu} \gets {\mathbf{V}^{\mu}_{d}}^{T}\mathbf{h} + \mathbf{b}^{\mu}_{d} $
\State $\mathbf{z}^{\sigma} \gets {\mathbf{V}^{\sigma}_{d}}^{T}\mathbf{h} + \mathbf{b}^{\sigma}_{d} $
\State $\boldsymbol{\alpha} \gets \text{softmax}(\mathbf{z}^{\alpha})$
\State $\boldsymbol{\mu} = \mathbf{z}^{\mu}$
\State $\boldsymbol{\sigma} \gets \text{exp}(\mathbf{z}^{\sigma})$
\State $\boldsymbol{\phi} \gets \frac{1}{2} \frac{(\boldsymbol{\mu}-\mathbf{x}_d)^2}{\boldsymbol{\sigma}^2} - \log \boldsymbol{\sigma} - \frac{1}{2}\log(2\pi)$
\State $\boldsymbol{\pi} \gets \frac{\boldsymbol{\alpha}\phi}{\sum_{j=1}^{K} \alpha_j \phi_j}$
\State $\partial z^{\alpha} \gets \boldsymbol{\pi} - \boldsymbol{\alpha}$
\State $\partial \boldsymbol{V}^{\alpha}_{d} \gets \partial z^{\alpha} \mathbf{h}$
\State $\partial \mathbf{b}^{\alpha}_{d} \gets \partial z^{\alpha}$
\State $\partial z^{\mu} \gets \boldsymbol{\pi}(x_d - \boldsymbol{\mu})/\boldsymbol{\sigma}^2$
\State $\partial \boldsymbol{V}^{\mu}_{d} \gets \partial z^{\mu} \mathbf{h}$
\State $\partial \mathbf{b}^{\mu}_{d} \gets \partial z^{\mu}$
\State $\partial z^{\sigma} \gets \boldsymbol{\pi}\left\{ (x_d - \boldsymbol{\mu})/\boldsymbol{\sigma}^2 -1\right\}$
\State $\partial \boldsymbol{V}^{\sigma}_{d} \gets \partial z^{\sigma} \mathbf{h}$
\State $\partial \mathbf{b}^{\sigma}_{d} \gets \partial z^{\sigma}$
\State $\partial \mathbf{h} \gets z^{\alpha}\boldsymbol{V}^{\alpha}_{d} + z^{\mu}\boldsymbol{V}^{\mu}_{d} + z^{\sigma}\boldsymbol{V}^{\sigma}_{d}$
\State $\partial \boldsymbol{\phi} = \partial \mathbf{h}\sigma(\boldsymbol{\psi})(1-\sigma(\boldsymbol{\psi}))$
\State $\partial \rho_d \gets \sum_{j} \partial \boldsymbol{\psi}_ja_j$
\State $\partial \mathbf{a} \gets \partial \mathbf{a} + \partial \boldsymbol{\psi}_{\rho}$
\State $\partial \mathbf{W}_{.,d} \gets \partial \mathbf{a}x_d$
\If {$d=1$}
	\State $\partial \mathbf{c} \gets \partial \mathbf{a}$
\Else
	\State $\mathbf{a} \gets \mathbf{a} - x_d \mathbf{W}_{.,d}$
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{RNN-RNADE gradients}
\begin{algorithmic}%[1]
\For{$t$ from $T$ to $1$}
\State $\mathbf{a} \gets \mathbf{c}$
\For{$d$ from $1$ to $D$}
	\State $\mathbf{a} \gets \mathbf{a} + x_{d}\mathbf{W}_{.,d} $
\EndFor
\For{$d$ from $D$ to $1$}
\State $\boldsymbol{\psi}_{t} \gets \rho_{d} \mathbf a$
\State $\mathbf{h}_{t} \gets \sigma(\boldsymbol{\psi}_t)$
\State $\mathbf{z}^{\alpha}_{t} \gets {\mathbf{V}^{\alpha}_{d}}^{T}\mathbf{h}_t + \mathbf{b}^{\alpha}_{d(t)} $
\State $\mathbf{z}^{\mu}_{t} \gets {\mathbf{V}^{\mu}_{d}}^{T}\mathbf{h}_t + \mathbf{b}^{\mu}_{d(t)} $
\State $\mathbf{z}^{\sigma}_{t} \gets {\mathbf{V}^{\sigma}_{d}}^{T}\mathbf{h}_t + \mathbf{b}^{\sigma}_{d(t)} $
\State $\boldsymbol{\alpha}_{t} \gets \text{softmax}(\mathbf{z}^{\alpha}_t)$
\State $\boldsymbol{\mu}_{t} = \mathbf{z}^{\mu}_t$
\State $\boldsymbol{\sigma}_{t} \gets \text{exp}(\mathbf{z}^{\sigma}_t)$
\State $\boldsymbol{\phi}_{t} \gets \frac{1}{2} \frac{(\boldsymbol{\mu}_t-\mathbf{x}^{t}_d)^2}{\boldsymbol{\sigma}^2} - \log \boldsymbol{\sigma}_t - \frac{1}{2}\log(2\pi)$
\State $\boldsymbol{\pi}_{t} \gets \frac{\boldsymbol{\alpha}_t\phi_t}{\sum_{j=1}^{K} \alpha_j \phi_j}$
\State $\partial z^{\alpha}_{t} \gets \boldsymbol{\pi}_t - \boldsymbol{\alpha}_t$
\State $\partial \boldsymbol{V}^{\alpha}_{d} \gets \partial z^{\alpha}_t \mathbf{h}_t$
\State $\partial \mathbf{b}^{\alpha}_{d(t)} \gets \partial z^{\alpha}_t$
\State $\partial z^{\mu}_{t} \gets \boldsymbol{\pi_t}(x_d - \boldsymbol{\mu}_t)/\boldsymbol{\sigma}_t^2$
\State $\partial \boldsymbol{V}^{\mu}_{d} \gets \partial z^{\mu}_t \mathbf{h}_t$
\State $\partial \mathbf{b}^{\mu}_{d(t)} \gets \partial z^{\mu}_t$
\State $\partial z^{\sigma}_t \gets \boldsymbol{\pi}_t\left\{ (x_d - \boldsymbol{\mu}_t)/\boldsymbol{\sigma}^2_t -1\right\}$
\State $\partial \boldsymbol{V}^{\sigma}_{d} \gets \partial z^{\sigma}_t \mathbf{h}_t$
\State $\partial \mathbf{b}^{\sigma}_{d(t)} \gets \partial z^{\sigma}_t$
\State $\partial \mathbf{h} \gets z^{\alpha}_t\boldsymbol{V}^{\alpha}_{d} + z^{\mu}_t\boldsymbol{V}^{\mu}_{d} + z^{\sigma}_t\boldsymbol{V}^{\sigma}_{d}$
\State $\partial \boldsymbol{\phi}_t = \partial \mathbf{h}_t\sigma(\boldsymbol{\psi}_t)(1-\sigma(\boldsymbol{\psi}_t))$
\State $\partial \rho_d(t) \gets \sum_{j} \partial \boldsymbol{\psi}_ja_j$
\State $\partial \mathbf{a} \gets \partial \mathbf{a} + \partial \boldsymbol{\psi}_{\rho}$
\State $\partial \mathbf{W}_{.,d} \gets \partial \mathbf{a}x_d$
\If {$d=1$}
	\State $\partial \mathbf{c} \gets \partial \mathbf{a}$
\Else
	\State $\mathbf{a} \gets \mathbf{a} - x^{t}_d \mathbf{W}_{.,d}$
\EndIf
\EndFor
\State $\partial W_{\alpha} \gets \partial W_{\alpha} + \partial \mathbf{b}^{\alpha}_{t}{\mathbf{h}^{t-1}}^T$
\State $\partial W_{\mu} \gets \partial W_{\mu} + \partial \mathbf{b}^{\mu}_{t}{\mathbf{h}^{t-1}}^T$
\State $\partial W_{\sigma} \gets \partial W_{\sigma} + \partial \mathbf{b}^{\sigma}_{t}{\mathbf{h}^{t-1}}^T$
\If {$t=T$}
	\State $\partial \mathbf{h}^{t} \gets  W_{\alpha} \partial \mathbf{b}^{\alpha}_{t+1} + W_{\mu} \partial \mathbf{b}^{\mu}_{t+1} + W_{\sigma} \partial \mathbf{b}^{\sigma}_{t+1}$
\Else
	\State $\partial \mathbf{h}^{t} \gets W_{rec}\partial  \mathbf{h}^{t+1}\mathbf{h}^{t+1}(1-\mathbf{h}^{t+1}) + W_{\alpha} \partial \mathbf{b}^{\alpha}_{t+1} + W_{\mu} \partial \mathbf{b}^{\mu}_{t+1} + W_{\sigma} \partial \mathbf{b}^{\sigma}_{t+1}$
\EndIf
\State $\partial \mathbf{b}_h \gets \partial \mathbf{b}_h + \partial \mathbf{h}^{t} \mathbf{h}^{t} (1-\mathbf{h}^{t})$
\State $\partial W_{rec} \gets \partial W_{rec} + \partial \mathbf{h}^{t} \mathbf{h}^{t} (1-\mathbf{h}^{t}){\mathbf{h}^{t-1}}^T$
\State $\partial W_{in} \gets \partial W_{in} + \partial \mathbf{h}^{t} \mathbf{h}^{t} (1-\mathbf{h}^{t})\mathbf{x}_t^T$
\EndFor
\end{algorithmic}
\end{algorithm}
\newpage
\section{2-D stochastic trajectory supplementary material}
The experiment described in Section 5.1 in the main text applied an RNN-RNADE to the problem of predicting the next point in a stochastic sequence. Enclosed with this document is the video,\\ {\it trajectory.m4v}, which shows an example unseen trajectory and predictions made by the RNN-RNADE. At each time-step \(T=t\), the RNN-RNADE produces 10 samples from the predicted distribution at \(T=t+1\) conditioned on \(T<t+1\). The ground truth (actual value at \(t+1\)) is depicted with a circle marker while the predictions are displayed with a cross. The means of the distribution clearly follow the path of the trajectory. 

 \section{Videos of bouncing balls supplementary material}
The experiment described in Section 5.3 in the main text applied an RNN-RNADE to the problem of predicting the pixels in the next frame of a video depicting 3 bouncing balls. The video was randomly generated and has not been previously seen by the RNN-RNADE. At each time-step \(T=t\), the RNN-RNADE produces 1 sample from the predicted distribution at \(T=t+1\), conditioned on \(T<t+1\). Enclosed with this document is the video, {\it bouncing.m4v}, which shows (top row left) the ground truth (actual frame at \(t+1\)), (top row middle) the frame sampled by the RNN-RNADE from the predicted distribution at  \(t+1\), (top row right) the absolute difference per pixel between the ground truth and the sampled value and (bottom) the sum of the squared errors between the ground truth and sampled pixels per frame. The video shows that the sampled frame is similar to the ground truth but there is a small amount of noise in space where there are no balls. There is also noise around the outline of the balls. Apart from the first frame, the highest error occurs where two of the balls overlap at \(t=69\) and \(t=110\), implying that the dynamics of this interaction have not been entirely captured by the model. Elsewhere, the sample error is low. The mean of the sum of the squared errors per frame on this sequence is 2.08. 

\newpage
\bibliography{bibliography}
\bibliographystyle{plain}


\end{document}
