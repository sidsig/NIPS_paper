@article{Bengio1994,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={Neural Networks, IEEE Transactions on},
  volume={5},
  number={2},
  pages={157--166},
  year={1994},
  publisher={IEEE}
}

@inproceedings{Boulanger-Lewandowski2012,
	Author = {N. Boulanger-Lewandowski and Y. Bengio and P. Vincent},
	Booktitle = {29th International Conference on Machine Learning},
	Address = {Edinburgh, Scotland, UK},
	Title = {Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription},
	Year = {2012}}

@article{Larochelle2011,
  title={The neural autoregressive distribution estimator},
  author={Larochelle, Hugo and Murray, Iain},
  journal={Journal of Machine Learning Research},
  volume={15},
  pages={29--37},
  year={2011}
}

@inproceedings{Martens2011,
  title={Learning recurrent neural networks with Hessian-free optimization},
  author={Martens, James and Sutskever, Ilya},
  booktitle={Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  pages={1033--1040},
  year={2011}
}

@inproceedings{Sutskever2008,
  title={The Recurrent Temporal Restricted Boltzmann Machine},
  author={Sutskever, Ilya and Hinton, Geoffrey E and Taylor, Graham W},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1601--1608},
  year={2008}
}

@article{Werbos1990,
  title={Backpropagation through time: what it does and how to do it},
  author={Werbos, Paul J},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1550--1560},
  year={1990},
  publisher={IEEE}
}

@inproceedings{Smaragdis2006,
	Address = {Whistler, Canada},
	Author = {P. Smaragdis and B. Raj and Ma. Shashanka},
	Booktitle = {Neural Information Processing Systems Workshop},
	Month = dec,
	Title = {A probabilistic latent variable model for acoustic modeling},
	Year = {2006}}

@INPROCEEDINGS{Boulanger-Lewandowski2013,
author={Boulanger-Lewandowski, N. and Bengio, Y. and Vincent, P.},
booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={High-dimensional sequence transduction},
year={2013},
month={May},
pages={3178--3182}}

@article{bengio2012advances,
  title={Advances in Optimizing Recurrent Networks},
  author={Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
  year={2012}
}

@inproceedings{AISTATS2011_Bengio11, 
    Publisher = {Journal of Machine Learning Research - Workshop and Conference Proceedings}, 
    Author = {Yoshua Bengio}, 
    Url = {http://www.jmlr.org/proceedings/papers/v15/bengio11a/bengio11a.pdf}, 
    Booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)}, 
    Title = {Discussion of ``{The Neural Autoregressive Distribution Estimator}"}, 
    Volume = {15}, 
    Editor = {Geoffrey J. Gordon and David B. Dunson}, 
    Year = {2011}, 
    Pages = {38-39} 
   }

@inproceedings{sutskever2007learning,
  title={Learning multilevel distributed representations for high-dimensional sequences},
  author={Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={548--555},
  year={2007}
}

@inproceedings{mikolov2011empirical,
  title={Empirical Evaluation and Combination of Advanced Language Modeling Techniques.},
  author={Mikolov, Tomas and Deoras, Anoop and Kombrink, Stefan and Burget, Lukas and Cernock{\`y}, Jan},
  booktitle={INTERSPEECH},
  pages={605--608},
  year={2011}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and others},
  volume={1},
  year={2006},
  publisher={springer New York}
}


@inproceedings{boulangerphone,
    author = "Boulanger-Lewandowski, Nicolas and Droppo, Jasha and Seltzer, Mike and Yu, Dong",
    title = "Phone sequence modeling with recurrent neural networks",
    year = "2014",
    month = may,
    booktitle = "Proceedings of the 2014 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2014), Florence, Italy"
}

@incollection{Uria2013,
title = {RNADE: The real-valued neural autoregressive density-estimator},
author = {Benigno Uria and Iain Murray and Hugo Larochelle},
booktitle = {Advances in Neural Information Processing Systems 26},
pages = {2175--2183},
year = {2013},
url = {http://www.benignouria.com/en/research/papers/Uria2013.pdf},
}

@techreport{rumelhart1985learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={DTIC Document}
}

@INPROCEEDINGS{bergstra+al:2010-scipy,
     author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
       note = {Oral Presentation},
   abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPyâ€™s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPyâ€™s syntax and semantics, while being statically typed and
functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates
them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6Ã— to 7.5Ã— faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the
CPU and between 6.5Ã— and 44Ã— faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.}
}

@article{theis2011all,
  title={In all likelihood, deep belief is not enough},
  author={Theis, Lucas and Gerwinn, Sebastian and Sinz, Fabian and Bethge, Matthias},
  journal={The Journal of Machine Learning Research},
  volume={12},
  pages={3071--3096},
  year={2011},
  publisher={JMLR. org}
}

@article{bishop1994mixture,
  title={Mixture density networks},
  author={Bishop, Christopher M},
  year={1994},
  publisher={Aston University}
}